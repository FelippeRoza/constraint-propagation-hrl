model:
  embed_dim: 512
  tiny_transformer_layers: 3
training:
  algo: PPO
  lr: 3e-4
  n_envs: 8 # Number of environments to run in parallel
  batch_size: 32 # Minibatch size for PPO
  n_steps: 512 # Number of steps to collect per environment before updating
  total_timesteps: 1_000_000 # Total number of timesteps to train for
environment:
  name: SafetyCarGoal2Vision-v0
  max_steps: 1000
