model:
  embed_dim: 512
  tiny_transformer_layers: 3
training:
  algo: PPO
  lr: 3e-4
  batch_size: 64 # Minibatch size for PPO
  n_steps: 2048 # Number of steps to collect per environment before updating
  total_timesteps: 1_000_000 # Total number of timesteps to train for
environment:
  name: SafetyCarGoal2Vision-v0
  max_steps: 1000
